{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantifying uncertainty is an important aspect of argumentation in the computational humanities when the measurements we take involve finite samples of data. This notebook explores two forms of confidence intervals for classification accuracy: binomial CIs and CIs calculated using the bootstrap.  We'll use them to generate confidence intervals for the accuracies in Underwood (2018), \"The Life Cycle of Genres\" using data available to support that article: [Underwood Github repo](https://github.com/tedunderwood/fiction).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "from math import sqrt \n",
    "from scipy.stats import norm\n",
    "from random import choices\n",
    "\n",
    "import random\n",
    "import csv\n",
    "from os import path\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_meta(filename, folder):\n",
    "\n",
    "    detective_tags=set(['det100'])\n",
    "    negative_tags = set(['random', 'chirandom'])\n",
    "    \n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    pos_ids={}\n",
    "    neg_ids={}\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        doc_id=row[\"docid\"]\n",
    "        author=row[\"author\"]\n",
    "        genres=set(x.strip() for x in row[\"genretags\"].split(\"|\"))\n",
    "        \n",
    "        if len(detective_tags.intersection(genres)) > 0 and path.exists(\"%s/%s.fic.tsv\" % (folder, doc_id)):\n",
    "            pos_ids[doc_id]=author\n",
    "\n",
    "        if len(negative_tags.intersection(genres)) > 0 and len(detective_tags.intersection(genres)) == 0  and path.exists(\"%s/%s.fic.tsv\" % (folder, doc_id)):\n",
    "            neg_ids[doc_id]=author\n",
    "\n",
    "    # Choose equal number of random documents for negative labels\n",
    "    negs=list(neg_ids.keys())\n",
    "    random.shuffle(negs)\n",
    "    neg_ids_final={}\n",
    "    for doc_id in negs[:len(pos_ids)]:\n",
    "        neg_ids_final[doc_id]=neg_ids[doc_id]\n",
    "    return pos_ids, neg_ids_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89 89\n"
     ]
    }
   ],
   "source": [
    "pos_ids, neg_ids=read_meta(\"../data/underwood_genre_data/finalmeta.csv\", \"../data/underwood_genre_data/newdata\")\n",
    "print(len(pos_ids), len(neg_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_vocab(filename):\n",
    "    vocab={}\n",
    "    df = pd.read_csv(filename)\n",
    "    for index, row in df.iterrows():\n",
    "            vocab[row[0]]=len(vocab)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=read_vocab(\"../data/underwood_genre_data/new10k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_to_ids(data, feature_vocab):\n",
    "\n",
    "    new_data=sparse.lil_matrix((len(data), len(feature_vocab)))\n",
    "    for idx,doc in enumerate(data):\n",
    "        for f in doc:\n",
    "            if f in feature_vocab:\n",
    "                new_data[idx,feature_vocab[f]]=doc[f]\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(folder, pos_ids, neg_ids, vocab):\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    M=[]\n",
    "    \n",
    "    for doc_id in pos_ids:\n",
    "        filename=\"%s/%s.fic.tsv\" % (folder, doc_id)\n",
    "        feats={}\n",
    "        with open(filename, encoding=\"utf-8\") as file:\n",
    "            for line in file:\n",
    "                cols=line.rstrip().split(\"\\t\")\n",
    "                word=cols[0]\n",
    "                # word count is here if we need it but we'll use a binary flag instead\n",
    "                count=int(cols[1])\n",
    "                feats[word]=1\n",
    "\n",
    "        X.append(feats)\n",
    "        Y.append(\"detective\")\n",
    "        M.append(pos_ids[doc_id])\n",
    "\n",
    "    for doc_id in neg_ids:\n",
    "        filename=\"%s/%s.fic.tsv\" % (folder, doc_id)\n",
    "        feats={}\n",
    "        with open(filename, encoding=\"utf-8\") as file:\n",
    "            for line in file:\n",
    "                cols=line.rstrip().split(\"\\t\")\n",
    "                word=cols[0]\n",
    "                # word count is here if we need it but we'll use a binary flag instead\n",
    "                count=int(cols[1])\n",
    "                feats[word]=1\n",
    "        \n",
    "        X.append(feats)\n",
    "        Y.append(\"random\")\n",
    "        M.append(neg_ids[doc_id])\n",
    "    \n",
    "    alldata=list(zip(X, Y, M))\n",
    "    random.shuffle(alldata)\n",
    "    shufX, shufY, shufM = zip(*alldata)\n",
    "    \n",
    "    trainX_ids=features_to_ids(shufX, vocab)\n",
    "\n",
    "    return trainX_ids, np.array(shufY), np.array(shufM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, M = read_data(\"../data/underwood_genre_data/newdata\", pos_ids, neg_ids, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.778\n",
      "Accuracy: 1.000\n",
      "Accuracy: 0.944\n",
      "Accuracy: 0.944\n",
      "Accuracy: 0.889\n",
      "Accuracy: 0.778\n",
      "Accuracy: 1.000\n",
      "Accuracy: 0.944\n",
      "Accuracy: 0.941\n",
      "Accuracy: 0.882\n"
     ]
    }
   ],
   "source": [
    "# We'll use GroupKFold to ensure that different texts by the same author don't appear in both the train \n",
    "# and test partition since it would be too easy (most works by the same author fall in the same genre)\n",
    "\n",
    "kf = GroupKFold(n_splits=10)\n",
    "\n",
    "predictions=[]\n",
    "truth=[]\n",
    "\n",
    "for train_index, test_index in kf.split(X, Y, M):\n",
    "    \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    clf = linear_model.LogisticRegression(C=1, solver='lbfgs', penalty='l2', max_iter=10000)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Accuracy: %.3f\" % clf.score(X_test, y_test))\n",
    "    predictions.extend(clf.predict(X_test))\n",
    "    truth.extend(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's calculate parametric confidence intervals; these are appropriate where the central limit theorem holds (e.g., for metrics that involve averaging, like accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binomial_confidence_intervals(predictions, truth, confidence_level=0.95):\n",
    "    correct=[]\n",
    "    for pred, gold in zip(predictions, truth):\n",
    "        correct.append(int(pred==gold))\n",
    "        \n",
    "    success_rate=np.mean(correct)\n",
    "\n",
    "    # two-tailed test\n",
    "    critical_value=(1-confidence_level)/2\n",
    "    # ppf finds z such that p(X < z) = critical_value\n",
    "    z_alpha=-1*norm.ppf(critical_value)\n",
    "    \n",
    "    # the standard error is the square root of the variance/sample size\n",
    "    # the variance for a binomial test is p*(1-p)\n",
    "    standard_error=sqrt((success_rate*(1-success_rate))/len(correct))\n",
    "\n",
    "    lower=success_rate-z_alpha*standard_error\n",
    "    upper=success_rate+z_alpha*standard_error\n",
    "    print(\"%.3f, %s%% Confidence interval: [%.3f,%.3f]\" % (success_rate, confidence_level*100, lower, upper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(truth, predictions):\n",
    "    correct=0.\n",
    "    for idx in range(len(truth)):\n",
    "        g=truth[idx]\n",
    "        p=predictions[idx]\n",
    "        if g == p:\n",
    "            correct+=1\n",
    "    return correct/len(truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.910, 95.0% Confidence interval: [0.868,0.952]\n"
     ]
    }
   ],
   "source": [
    "binomial_confidence_intervals(predictions, truth, confidence_level=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's calculate confidence intervals using the bootstrap, which doesn't require assumptions on the parametric form.  This means it can be used for just about any metric (including those that don't involve averaging -- like the F-score).  How do these bounds compare to the parametric ones for accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(gold, predictions, metric, B=10000, confidence_level=0.95):\n",
    "    critical_value=(1-confidence_level)/2\n",
    "    lower_sig=100*critical_value\n",
    "    upper_sig=100*(1-critical_value)\n",
    "    data=[]\n",
    "    for g, p in zip(gold, predictions):\n",
    "        data.append([g,p])\n",
    "\n",
    "    accuracies=[]\n",
    "    \n",
    "    for b in range(B):\n",
    "        choice=choices(data, k=len(data))\n",
    "        choice=np.array(choice)\n",
    "        accuracy=metric(choice[:,0], choice[:,1])\n",
    "        \n",
    "        accuracies.append(accuracy)\n",
    "    \n",
    "    percentiles=np.percentile(accuracies, [lower_sig, 50, upper_sig])\n",
    "    \n",
    "    lower=percentiles[0]\n",
    "    median=percentiles[1]\n",
    "    upper=percentiles[2]\n",
    "    \n",
    "    return lower, median, upper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.910, 95.0% Bootstrap confidence interval: [0.865, 0.949]\n"
     ]
    }
   ],
   "source": [
    "confidence_level=0.95\n",
    "lower, median,upper=bootstrap(truth, predictions, accuracy, B=10000,confidence_level=confidence_level)\n",
    "print(\"%.3f, %s%% Bootstrap confidence interval: [%.3f, %.3f]\" % (median, confidence_level*100, lower, upper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
