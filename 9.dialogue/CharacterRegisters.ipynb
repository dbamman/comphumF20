{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores the degree to which different characters have measurably different *registers* by training a multiclass classifier on character dialogue to predict the speaker.  This notebooks works with the output of [BookNLP](https://github.com/dbamman/book-nlp), which recognizes quotations and carries out speaker attribution on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from collections import Counter\n",
    "import math\n",
    "from os import path\n",
    "import json\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from scipy import sparse\n",
    "import nltk\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import gzip\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_speakers(filename, top_n=10):\n",
    "        \n",
    "    with gzip.open(filename) as file:\n",
    "        data=json.load(file)\n",
    "        counts={}\n",
    "\n",
    "        for character in data[\"characters\"]:\n",
    "            char_id=character[\"id\"]\n",
    "\n",
    "            gender=character[\"g\"]\n",
    "            names='; '.join([x[\"n\"] for x in character[\"names\"]])\n",
    "            quotes=0\n",
    "            for q in character[\"speaking\"]:\n",
    "                quotes+=1\n",
    "            counts[(char_id, names)]=quotes\n",
    "\n",
    "        sorted_x = sorted(counts.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        print(\"#quotes\\tchar_id\\tname\")\n",
    "        for (charid,name),v in sorted_x[:top_n]:\n",
    "            print(\"%s\\t%s\\t%s\" % (v,charid, name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first just examine the characters who have the most dialogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_top_speakers(\"../data/harry_potter.book.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_top_speakers(\"../data/lotr.book.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quote_data(data, targets, max_num):\n",
    "            \n",
    "    X=[]\n",
    "    Y=[]\n",
    "    \n",
    "    for character in data[\"characters\"]:\n",
    "        proper_name_count=character[\"NNPcount\"]\n",
    "        char_id=character[\"id\"]\n",
    "        if char_id in targets:\n",
    "            name=targets[char_id]\n",
    "            quotes=[]\n",
    "            for q in character[\"speaking\"]:\n",
    "                quote=q[\"w\"].lower()\n",
    "                tokens=nltk.word_tokenize(quote)\n",
    "                quotes.append(tokens)\n",
    "            \n",
    "            random.shuffle(quotes)\n",
    "            \n",
    "            assert len(quotes) >= max_num\n",
    "                \n",
    "            X.extend(quotes[:max_num])\n",
    "            Y.extend([name]*max_num)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename, targets, max_num):\n",
    "\n",
    "    with gzip.open(filename) as file:\n",
    "        data=json.load(file)\n",
    "        X, Y=get_quote_data(data, targets, max_num)\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features(dataX, feature_functions):\n",
    "    \n",
    "    \"\"\" This function featurizes the data according to the list of parameter feature_functions \"\"\"\n",
    "    \n",
    "    data=[]\n",
    "    for tokens in dataX:\n",
    "        feats={}\n",
    "        \n",
    "        for function in feature_functions:\n",
    "            feats.update(function(tokens))\n",
    "\n",
    "        data.append(feats)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_to_ids(data, feature_vocab):\n",
    "    \n",
    "    \"\"\" \n",
    "    \n",
    "    This helper function converts a dictionary of feature names to a sparse representation\n",
    " that we can fit in a scikit-learn model.  This is important because almost all feature \n",
    " values will be 0 for most documents (note: why?), and we don't want to save them all in \n",
    " memory.\n",
    "\n",
    "    \"\"\"\n",
    "    new_data=sparse.lil_matrix((len(data), len(feature_vocab)))\n",
    "    for idx,doc in enumerate(data):\n",
    "        for f in doc:\n",
    "            if f in feature_vocab:\n",
    "                new_data[idx,feature_vocab[f]]=doc[f]\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(data, top_n=None):\n",
    "    \n",
    "    \"\"\" \n",
    "    \n",
    "    This helper function converts a dictionary of feature names to unique numerical ids. \n",
    "    top_n limits the features to only the n most frequent features observed in the training data \n",
    "    (in terms of the number of documents that contains it).\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    counts=Counter()\n",
    "    for doc in data:\n",
    "        for feat in doc:\n",
    "            counts[feat]+=1\n",
    "\n",
    "    feature_vocab={}\n",
    "\n",
    "    for idx, (k, v) in enumerate(counts.most_common(top_n)):\n",
    "        feature_vocab[k]=idx\n",
    "                \n",
    "    return feature_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(trainX, devX, trainY, devY, feature_functions):\n",
    "\n",
    "    \"\"\" This function evaluates a list of feature functions on the training/dev data arguments \"\"\"\n",
    "    \n",
    "    trainX_feat=build_features(trainX, feature_functions)\n",
    "    devX_feat=build_features(devX, feature_functions)\n",
    "\n",
    "    # just create vocabulary from features in *training* data.\n",
    "    feature_vocab=create_vocab(trainX_feat, top_n=100000)\n",
    "\n",
    "    trainX_ids=features_to_ids(trainX_feat, feature_vocab)\n",
    "    devX_ids=features_to_ids(devX_feat, feature_vocab)\n",
    "    \n",
    "    clf = linear_model.LogisticRegression(C=1, solver='lbfgs', penalty='l2', max_iter=10000)\n",
    "    clf.fit(trainX_ids, trainY)\n",
    "    \n",
    "    predictions=clf.predict(devX_ids)\n",
    "    \n",
    "    return clf, feature_vocab, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_class(trainY, devY):\n",
    "    labelCounts=Counter()\n",
    "    for label in trainY:\n",
    "        labelCounts[label]+=1\n",
    "    majority_class=labelCounts.most_common(1)[0][0]\n",
    "    \n",
    "    return [majority_class]*len(devY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_weights(clf, vocab, n=10):\n",
    "\n",
    "    reverse_vocab=[None]*len(clf.coef_[0])\n",
    "    for k in vocab:\n",
    "        reverse_vocab[vocab[k]]=k\n",
    "        \n",
    "    for i, cat in enumerate(clf.classes_):\n",
    "        \n",
    "        weights=clf.coef_[i]\n",
    "\n",
    "        for feature, weight in list(reversed(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))))[:n]:\n",
    "            print(\"%s\\t%.3f\\t%s\" % (cat, weight, feature))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_feature(tokens):\n",
    "    feats={}\n",
    "    for word in tokens:\n",
    "        feats[\"UNIGRAM_%s\" % word]=1\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preps={}\n",
    "with open(\"../data/preposition_list.txt\") as file:\n",
    "    for line in file:\n",
    "        if not line.startswith(\"#\"):\n",
    "            preps[line.rstrip()]=1\n",
    "            \n",
    "def preposition_feature(tokens):\n",
    "    feats={}\n",
    "    for word in tokens:\n",
    "        if word in preps:\n",
    "            feats[\"PREP_%s\" % word]=1\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_feature(tokens):\n",
    "    feats={}\n",
    "    feats[\"utterance_length\"]=len(tokens)\n",
    "    \n",
    "    avg_word_length=0.\n",
    "    for word in tokens:\n",
    "        avg_word_length+=len(word)\n",
    "    avg_word_length/=len(tokens)\n",
    "    \n",
    "    feats[\"avg_word_length\"]=avg_word_length\n",
    "    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readability_feature(tokens):\n",
    "    feats={}\n",
    "    data=' '.join(tokens)\n",
    "    feats[\"flesch_reading_ease\"]=textstat.flesch_reading_ease(data)\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punctuation_feature(tokens):\n",
    "    punct=set([\"?\", \",\", \".\", \"!\", \";\", \":\"])\n",
    "    feats={}\n",
    "    for word in tokens:\n",
    "        if word in punct:\n",
    "            feats[\"PUNCT_%s\" % word]=1\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(targets, features, filename, max_num):\n",
    "\n",
    "    random.seed(1)\n",
    "\n",
    "    X, Y=read_data(filename, targets, max_num=max_num)\n",
    "    X=np.array(X, dtype=object)\n",
    "    Y=np.array(Y, dtype=object)\n",
    "    kf = StratifiedKFold(n_splits=10, random_state=0, shuffle=True)\n",
    "    \n",
    "    preds=[]\n",
    "    golds=[]\n",
    "    baseline=[]\n",
    "    \n",
    "    for train_index, test_index in kf.split(X, Y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = Y[train_index], Y[test_index]\n",
    "        clf, vocab, predictions=pipeline(X_train, X_test, y_train, y_test, features)\n",
    "        preds.extend(predictions)\n",
    "        golds.extend(y_test)\n",
    "        baseline.extend(majority_class(y_train, y_test))\n",
    "    \n",
    "    print(\"Majority class: %.3f (%s)\\n\" % (accuracy_score(baseline, golds), len(golds)))\n",
    "    print(\"Cross-validated accuracy: %.3f (%s)\\n\" % (accuracy_score(preds, golds), len(golds)))\n",
    "\n",
    "    # print weights from last fold\n",
    "    print_weights(clf, vocab, n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's train a classifier to predict the character identity based on the *content* of their dialogue (effectively learning if different characters talk about kinds of different things).  How do the most characteristic features for each character accord with your own understanding of their language?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets={216: \"Frodo\", 106: \"Sam\", 49: \"Gandalf\", 317: \"Gimli\", 259: \"Legolas\"}\n",
    "process(targets, [unigram_feature], \"../data/lotr.book.gz\", 230)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets={343: \"Harry\", 247: \"Ron\", 302: \"Hermione\", 352: \"Dumbledore\", 298: \"Hagrid\"}\n",
    "process(targets, [unigram_feature], \"../data/harry_potter.book.gz\", 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's train a classifier on primarily *stylistic* features (average word length, average utterance length, frequency of specific punctuation, reading difficulty).  Can we still see measurable differences between characters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets={216: \"Frodo\", 106: \"Sam\", 49: \"Gandalf\", 317: \"Gimli\", 259: \"Legolas\"}\n",
    "process(targets, [length_feature, readability_feature, punctuation_feature], \"../data/lotr.book.gz\", 230)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets={343: \"Harry\", 247: \"Ron\", 302: \"Hermione\", 352: \"Dumbledore\", 298: \"Hagrid\"}\n",
    "process(targets, [length_feature, readability_feature, punctuation_feature], \"../data/harry_potter.book.gz\", 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given these results, let's brainstorm two ideas:\n",
    "\n",
    "* How could we use these distinctive voices to build a better system for speaker attribution?\n",
    "* How could we use these results to build a model for *free indirect discourse*? (i.e., where a character's voice influences the narration.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
